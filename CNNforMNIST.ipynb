{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "이번에는 이미지 인식 분야에서 가장 성공적으로 쓰이고 있는 Convolutional Neural Networks를 실습해본다.  \n",
    "Convolutional Neural Networks, CNN은 아래와 같은 Convolutional Layer를 여러층 가진 딥러닝 모델을 뜻한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 간단한 구조를 가진 CNN을 구현한다.\n",
    "MNIST 데이터를 이용하여 학습하고 테스팅 해본다. \n",
    "\n",
    "사용된 기술\n",
    "\n",
    "- conv2d\n",
    "- max-pool\n",
    "- weight 초기화: truncated normal dist.\n",
    "- RMSProp optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one epoch에 55,000장의 이미지중 랜덤하게 128개를 선택해서 사용 한다.\n",
    "- 메모리 문제로 10,000개의 testing image중에서 랜덤 shuffling을 한 후에 200개를 선택해서 사용 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 125\n",
    "test_size = 200\n",
    "training_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**구현 하는 Network**\n",
    "<img src = 'mycnn.JPG'>\n",
    "(출처: 직접 그림)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0, training error: 0.6364, training accuracy: 0.8153\n",
      "Testing Accuracy: 0.9700\n",
      "Epoch: 1, training error: 0.0272, training accuracy: 0.9928\n",
      "Testing Accuracy: 0.9900\n",
      "Epoch: 2, training error: 0.0138, training accuracy: 0.9966\n",
      "Testing Accuracy: 0.9800\n",
      "Epoch: 3, training error: 0.0104, training accuracy: 0.9973\n",
      "Testing Accuracy: 0.9950\n",
      "Epoch: 4, training error: 0.0080, training accuracy: 0.9980\n",
      "Testing Accuracy: 0.9950\n",
      "Epoch: 5, training error: 0.0064, training accuracy: 0.9986\n",
      "Testing Accuracy: 0.9900\n",
      "Epoch: 6, training error: 0.0052, training accuracy: 0.9988\n",
      "Testing Accuracy: 0.9900\n",
      "Epoch: 7, training error: 0.0046, training accuracy: 0.9990\n",
      "Testing Accuracy: 0.9900\n",
      "Epoch: 8, training error: 0.0043, training accuracy: 0.9989\n",
      "Testing Accuracy: 0.9900\n",
      "Epoch: 9, training error: 0.0037, training accuracy: 0.9991\n",
      "Testing Accuracy: 0.9950\n",
      "Epoch: 10, training error: 0.0031, training accuracy: 0.9994\n",
      "Testing Accuracy: 0.9950\n",
      "Epoch: 11, training error: 0.0029, training accuracy: 0.9993\n",
      "Testing Accuracy: 0.9950\n",
      "Epoch: 12, training error: 0.0028, training accuracy: 0.9992\n",
      "Testing Accuracy: 0.9900\n",
      "Epoch: 13, training error: 0.0022, training accuracy: 0.9995\n",
      "Testing Accuracy: 0.9900\n",
      "Epoch: 14, training error: 0.0021, training accuracy: 0.9995\n",
      "Testing Accuracy: 1.0000\n",
      "Early stop..\n",
      "time elapsed: 78.58s\n"
     ]
    }
   ],
   "source": [
    "def init_weights(shape):\n",
    "    #return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "\n",
    "# Filter weight vectors 또는 kernel: w, w2, w3, w4, w_0\n",
    "def model(X, w, w2, w3, w4, w_o, p_keep_conv, p_keep_hidden):\n",
    "        \n",
    "    l1a = tf.nn.relu(tf.nn.conv2d(X, w,                       # l1a shape=(?, 28, 28, 32)\n",
    "                        strides=[1, 1, 1, 1], padding='SAME'))\n",
    "    l1 = tf.nn.max_pool(l1a, ksize=[1, 2, 2, 1],              # l1 shape=(?, 14, 14, 32)\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "    l1 = tf.nn.dropout(l1, p_keep_conv)\n",
    "\n",
    "\n",
    "    l2a = tf.nn.relu(tf.nn.conv2d(l1, w2,                     # l2a shape=(?, 14, 14, 64)\n",
    "                        strides=[1, 1, 1, 1], padding='SAME'))\n",
    "    l2 = tf.nn.max_pool(l2a, ksize=[1, 2, 2, 1],              # l2 shape=(?, 7, 7, 64)\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "    l2 = tf.nn.dropout(l2, p_keep_conv)\n",
    "\n",
    "\n",
    "    l3a = tf.nn.relu(tf.nn.conv2d(l2, w3,                     # l3a shape=(?, 7, 7, 128)\n",
    "                        strides=[1, 1, 1, 1], padding='SAME'))\n",
    "    l3 = tf.nn.max_pool(l3a, ksize=[1, 2, 2, 1],              # l3 shape=(?, 4, 4, 128)\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "    l3 = tf.reshape(l3, [-1, w4.get_shape().as_list()[0]])    # reshape to (?, 2048)\n",
    "    l3 = tf.nn.dropout(l3, p_keep_conv)\n",
    "\n",
    "\n",
    "    l4 = tf.nn.relu(tf.matmul(l3, w4))\n",
    "    l4 = tf.nn.dropout(l4, p_keep_hidden)\n",
    "\n",
    "    pyx = tf.matmul(l4, w_o)\n",
    "    return pyx\n",
    "\n",
    "# Read data\n",
    "mnist = input_data.read_data_sets(\"MNIST_DATA/\", one_hot=True)\n",
    "#trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
    "\n",
    "# trx.reshape( n-inputs, image size, image size, depth )\n",
    " # this variable is input in model()\n",
    "#trX = trX.reshape(-1, 28, 28, 1)  # 28x28x1 input img\n",
    "#teX = teX.reshape(-1, 28, 28, 1)  # 28x28x1 input img\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "Y = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "w = init_weights([3, 3, 1, 32])       # 3x3x1 conv, 32 outputs\n",
    "w2 = init_weights([3, 3, 32, 64])     # 3x3x32 conv, 64 outputs\n",
    "w3 = init_weights([3, 3, 64, 128])    # 3x3x32 conv, 128 outputs\n",
    "w4 = init_weights([128 * 4 * 4, 625]) # FC 128 * 4 * 4 inputs, 625 outputs\n",
    "w_o = init_weights([625, 10])         # FC 625 inputs, 10 outputs (labels)\n",
    "\n",
    "p_keep_conv = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "\n",
    "py_x = model(X, w, w2, w3, w4, w_o, p_keep_conv, p_keep_hidden)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))\n",
    "train_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "predict_op = tf.argmax(py_x, 1)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # you need to initialize all variables\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        avg_training_accuracy = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "              \n",
    "        for step in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            batch_xs_image = batch_xs.reshape(-1, 28, 28, 1)\n",
    "            \n",
    "            sess.run(train_op, feed_dict={X: batch_xs_image, Y: batch_ys,\n",
    "                                          p_keep_conv: 0.8, p_keep_hidden: 0.5})\n",
    "            # Training average cost 계산\n",
    "            avg_cost += sess.run(cost, feed_dict={X: batch_xs_image, Y: batch_ys, p_keep_conv:1.0, p_keep_hidden:1.0})/total_batch\n",
    "            \n",
    "            avg_training_accuracy += (np.mean(np.argmax(batch_ys, axis=1) ==\n",
    "                         sess.run(predict_op, feed_dict={X: batch_xs_image,\n",
    "                                                        Y: batch_ys,\n",
    "                                                        p_keep_conv: 1.0,\n",
    "                                                        p_keep_hidden: 1.0})))/total_batch\n",
    "            \n",
    "        print(\"Epoch: %d, training error: %.4f, training accuracy: %.4f\"%(i,avg_cost,avg_training_accuracy))\n",
    "\n",
    "        # testing accuracy 계산\n",
    "        # 인덱스를 뒤 썩어 준다. 랜덤하게 200개 추출을 위해서\n",
    "        test_indices = np.arange(mnist.test.labels.shape[0]) # Get A Test Batch\n",
    "        np.random.shuffle(test_indices)\n",
    "        test_indices = test_indices[0:test_size] # 200개만 선택한다. \n",
    "        teX = mnist.test.images[test_indices].reshape(-1, 28, 28, 1) # input을 2차원 image를 담은 3차원 matrix로 표현 \n",
    "        testing_accuracy = np.mean(np.argmax(mnist.test.labels[test_indices], axis=1) ==\n",
    "                         sess.run(predict_op, feed_dict={X: teX,\n",
    "                                                        Y: mnist.test.labels[test_indices],\n",
    "                                                        p_keep_conv: 1.0,\n",
    "                                                        p_keep_hidden: 1.0}))\n",
    "        print(\"Testing Accuracy: %.4f\"%(testing_accuracy))\n",
    "        \n",
    "        # shuffled testing data 200개에 대해서 accuracy 1.0에 도달하면 Training을 멈춘다.\n",
    "        if testing_accuracy == 1.0:\n",
    "            print(\"Early stop..\")\n",
    "            break\n",
    "        \n",
    "    print(\"time elapsed: {:.2f}s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "mnist = input_data.read_data_sets(\"MNIST_DATA/\", one_hot=True)\n",
    "batch_xs, batch_ys = mnist.train.next_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 9997, 9998, 9999])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_indices = np.arange(mnist.test.labels.shape[0]) # Get A Test Batch\n",
    "#np.random.shuffle(test_indices)\n",
    "test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "total_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6,\n",
       "       5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5,\n",
       "       1, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4, 6, 4, 3,\n",
       "       0, 7, 0, 2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3, 6, 1, 3, 6,\n",
       "       9, 3, 1, 4, 1, 7, 6, 9, 6, 0, 5, 4, 9, 9, 2, 1, 9, 4, 8, 7, 3, 9, 7,\n",
       "       4, 4, 4, 9, 2, 5, 4, 7, 6, 7, 9, 0, 5, 8, 5, 6, 6, 5, 7, 8, 1, 0, 1,\n",
       "       6, 4, 6, 7, 3, 1, 7, 1, 8, 2, 0, 2, 9, 9, 5, 5, 1, 5, 6, 0, 3, 4, 4,\n",
       "       6, 5, 4, 6, 5, 4, 5, 1, 4, 4, 7, 2, 3, 2, 7, 1, 8, 1, 8, 1, 8, 5, 0,\n",
       "       8, 9, 2, 5, 0, 1, 1, 1, 0, 9, 0, 3, 1, 6, 4, 2])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(mnist.test.labels[0:200], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 7, 6, 6, 9, 5, 6, 6, 9, 9, 0, 6, 6, 6, 4, 0, 4, 4, 9, 9, 2, 0,\n",
       "       0, 0, 4, 5, 9, 8, 6, 2, 9, 3, 9, 2, 2, 6, 1, 7, 3, 3, 3, 8, 6, 7, 6,\n",
       "       9, 7, 9, 8, 0, 6, 0, 3, 7, 9, 5, 8, 2, 1, 1, 6, 6, 5, 8, 1, 3, 4, 5,\n",
       "       4, 8, 6, 7, 3, 5, 5, 1, 7, 1, 1, 0, 0, 5, 1, 2, 2, 3, 6, 9, 6, 8, 4,\n",
       "       6, 8, 0, 5, 4, 8, 8, 2, 7, 9, 7, 0, 6, 9, 8, 6, 9, 4, 7, 9, 4, 4, 4,\n",
       "       4, 6, 7, 1, 7, 3, 8, 0, 9, 5, 9, 4, 5, 1, 8, 4, 7, 8, 0, 9, 3, 2, 7,\n",
       "       3, 1, 9, 4, 1, 9, 1, 4, 2, 5, 1, 8, 3, 9, 1, 4, 9, 7, 7, 1, 6, 9, 5,\n",
       "       3, 6, 0, 7, 1, 4, 8, 9, 3, 8, 5, 8, 2, 0, 7, 4, 8, 6, 0, 3, 4, 3, 0,\n",
       "       2, 3, 3, 1, 8, 5, 4, 3, 1, 1, 1, 2, 4, 7, 7, 6])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(mnist.test.labels[test_indices], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4cbd5ec0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.test.images[4].reshape(28,28), cmap=\"gray\", interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "X_image = tf.reshape(batch_xs, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7fcfd2564c48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_batch = zip(range(0, 50000, 128),\n",
    "                     range(128, 50000+1, 128))\n",
    "training_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 128\n",
      "128 256\n",
      "256 384\n",
      "384 512\n",
      "512 640\n",
      "640 768\n",
      "768 896\n",
      "896 1024\n",
      "1024 1152\n",
      "1152 1280\n",
      "1280 1408\n",
      "1408 1536\n",
      "1536 1664\n",
      "1664 1792\n",
      "1792 1920\n",
      "1920 2048\n",
      "2048 2176\n",
      "2176 2304\n",
      "2304 2432\n",
      "2432 2560\n",
      "2560 2688\n",
      "2688 2816\n",
      "2816 2944\n",
      "2944 3072\n",
      "3072 3200\n",
      "3200 3328\n",
      "3328 3456\n",
      "3456 3584\n",
      "3584 3712\n",
      "3712 3840\n",
      "3840 3968\n",
      "3968 4096\n",
      "4096 4224\n",
      "4224 4352\n",
      "4352 4480\n",
      "4480 4608\n",
      "4608 4736\n",
      "4736 4864\n",
      "4864 4992\n",
      "4992 5120\n",
      "5120 5248\n",
      "5248 5376\n",
      "5376 5504\n",
      "5504 5632\n",
      "5632 5760\n",
      "5760 5888\n",
      "5888 6016\n",
      "6016 6144\n",
      "6144 6272\n",
      "6272 6400\n",
      "6400 6528\n",
      "6528 6656\n",
      "6656 6784\n",
      "6784 6912\n",
      "6912 7040\n",
      "7040 7168\n",
      "7168 7296\n",
      "7296 7424\n",
      "7424 7552\n",
      "7552 7680\n",
      "7680 7808\n",
      "7808 7936\n",
      "7936 8064\n",
      "8064 8192\n",
      "8192 8320\n",
      "8320 8448\n",
      "8448 8576\n",
      "8576 8704\n",
      "8704 8832\n",
      "8832 8960\n",
      "8960 9088\n",
      "9088 9216\n",
      "9216 9344\n",
      "9344 9472\n",
      "9472 9600\n",
      "9600 9728\n",
      "9728 9856\n",
      "9856 9984\n",
      "9984 10112\n",
      "10112 10240\n",
      "10240 10368\n",
      "10368 10496\n",
      "10496 10624\n",
      "10624 10752\n",
      "10752 10880\n",
      "10880 11008\n",
      "11008 11136\n",
      "11136 11264\n",
      "11264 11392\n",
      "11392 11520\n",
      "11520 11648\n",
      "11648 11776\n",
      "11776 11904\n",
      "11904 12032\n",
      "12032 12160\n",
      "12160 12288\n",
      "12288 12416\n",
      "12416 12544\n",
      "12544 12672\n",
      "12672 12800\n",
      "12800 12928\n",
      "12928 13056\n",
      "13056 13184\n",
      "13184 13312\n",
      "13312 13440\n",
      "13440 13568\n",
      "13568 13696\n",
      "13696 13824\n",
      "13824 13952\n",
      "13952 14080\n",
      "14080 14208\n",
      "14208 14336\n",
      "14336 14464\n",
      "14464 14592\n",
      "14592 14720\n",
      "14720 14848\n",
      "14848 14976\n",
      "14976 15104\n",
      "15104 15232\n",
      "15232 15360\n",
      "15360 15488\n",
      "15488 15616\n",
      "15616 15744\n",
      "15744 15872\n",
      "15872 16000\n",
      "16000 16128\n",
      "16128 16256\n",
      "16256 16384\n",
      "16384 16512\n",
      "16512 16640\n",
      "16640 16768\n",
      "16768 16896\n",
      "16896 17024\n",
      "17024 17152\n",
      "17152 17280\n",
      "17280 17408\n",
      "17408 17536\n",
      "17536 17664\n",
      "17664 17792\n",
      "17792 17920\n",
      "17920 18048\n",
      "18048 18176\n",
      "18176 18304\n",
      "18304 18432\n",
      "18432 18560\n",
      "18560 18688\n",
      "18688 18816\n",
      "18816 18944\n",
      "18944 19072\n",
      "19072 19200\n",
      "19200 19328\n",
      "19328 19456\n",
      "19456 19584\n",
      "19584 19712\n",
      "19712 19840\n",
      "19840 19968\n",
      "19968 20096\n",
      "20096 20224\n",
      "20224 20352\n",
      "20352 20480\n",
      "20480 20608\n",
      "20608 20736\n",
      "20736 20864\n",
      "20864 20992\n",
      "20992 21120\n",
      "21120 21248\n",
      "21248 21376\n",
      "21376 21504\n",
      "21504 21632\n",
      "21632 21760\n",
      "21760 21888\n",
      "21888 22016\n",
      "22016 22144\n",
      "22144 22272\n",
      "22272 22400\n",
      "22400 22528\n",
      "22528 22656\n",
      "22656 22784\n",
      "22784 22912\n",
      "22912 23040\n",
      "23040 23168\n",
      "23168 23296\n",
      "23296 23424\n",
      "23424 23552\n",
      "23552 23680\n",
      "23680 23808\n",
      "23808 23936\n",
      "23936 24064\n",
      "24064 24192\n",
      "24192 24320\n",
      "24320 24448\n",
      "24448 24576\n",
      "24576 24704\n",
      "24704 24832\n",
      "24832 24960\n",
      "24960 25088\n",
      "25088 25216\n",
      "25216 25344\n",
      "25344 25472\n",
      "25472 25600\n",
      "25600 25728\n",
      "25728 25856\n",
      "25856 25984\n",
      "25984 26112\n",
      "26112 26240\n",
      "26240 26368\n",
      "26368 26496\n",
      "26496 26624\n",
      "26624 26752\n",
      "26752 26880\n",
      "26880 27008\n",
      "27008 27136\n",
      "27136 27264\n",
      "27264 27392\n",
      "27392 27520\n",
      "27520 27648\n",
      "27648 27776\n",
      "27776 27904\n",
      "27904 28032\n",
      "28032 28160\n",
      "28160 28288\n",
      "28288 28416\n",
      "28416 28544\n",
      "28544 28672\n",
      "28672 28800\n",
      "28800 28928\n",
      "28928 29056\n",
      "29056 29184\n",
      "29184 29312\n",
      "29312 29440\n",
      "29440 29568\n",
      "29568 29696\n",
      "29696 29824\n",
      "29824 29952\n",
      "29952 30080\n",
      "30080 30208\n",
      "30208 30336\n",
      "30336 30464\n",
      "30464 30592\n",
      "30592 30720\n",
      "30720 30848\n",
      "30848 30976\n",
      "30976 31104\n",
      "31104 31232\n",
      "31232 31360\n",
      "31360 31488\n",
      "31488 31616\n",
      "31616 31744\n",
      "31744 31872\n",
      "31872 32000\n",
      "32000 32128\n",
      "32128 32256\n",
      "32256 32384\n",
      "32384 32512\n",
      "32512 32640\n",
      "32640 32768\n",
      "32768 32896\n",
      "32896 33024\n",
      "33024 33152\n",
      "33152 33280\n",
      "33280 33408\n",
      "33408 33536\n",
      "33536 33664\n",
      "33664 33792\n",
      "33792 33920\n",
      "33920 34048\n",
      "34048 34176\n",
      "34176 34304\n",
      "34304 34432\n",
      "34432 34560\n",
      "34560 34688\n",
      "34688 34816\n",
      "34816 34944\n",
      "34944 35072\n",
      "35072 35200\n",
      "35200 35328\n",
      "35328 35456\n",
      "35456 35584\n",
      "35584 35712\n",
      "35712 35840\n",
      "35840 35968\n",
      "35968 36096\n",
      "36096 36224\n",
      "36224 36352\n",
      "36352 36480\n",
      "36480 36608\n",
      "36608 36736\n",
      "36736 36864\n",
      "36864 36992\n",
      "36992 37120\n",
      "37120 37248\n",
      "37248 37376\n",
      "37376 37504\n",
      "37504 37632\n",
      "37632 37760\n",
      "37760 37888\n",
      "37888 38016\n",
      "38016 38144\n",
      "38144 38272\n",
      "38272 38400\n",
      "38400 38528\n",
      "38528 38656\n",
      "38656 38784\n",
      "38784 38912\n",
      "38912 39040\n",
      "39040 39168\n",
      "39168 39296\n",
      "39296 39424\n",
      "39424 39552\n",
      "39552 39680\n",
      "39680 39808\n",
      "39808 39936\n",
      "39936 40064\n",
      "40064 40192\n",
      "40192 40320\n",
      "40320 40448\n",
      "40448 40576\n",
      "40576 40704\n",
      "40704 40832\n",
      "40832 40960\n",
      "40960 41088\n",
      "41088 41216\n",
      "41216 41344\n",
      "41344 41472\n",
      "41472 41600\n",
      "41600 41728\n",
      "41728 41856\n",
      "41856 41984\n",
      "41984 42112\n",
      "42112 42240\n",
      "42240 42368\n",
      "42368 42496\n",
      "42496 42624\n",
      "42624 42752\n",
      "42752 42880\n",
      "42880 43008\n",
      "43008 43136\n",
      "43136 43264\n",
      "43264 43392\n",
      "43392 43520\n",
      "43520 43648\n",
      "43648 43776\n",
      "43776 43904\n",
      "43904 44032\n",
      "44032 44160\n",
      "44160 44288\n",
      "44288 44416\n",
      "44416 44544\n",
      "44544 44672\n",
      "44672 44800\n",
      "44800 44928\n",
      "44928 45056\n",
      "45056 45184\n",
      "45184 45312\n",
      "45312 45440\n",
      "45440 45568\n",
      "45568 45696\n",
      "45696 45824\n",
      "45824 45952\n",
      "45952 46080\n",
      "46080 46208\n",
      "46208 46336\n",
      "46336 46464\n",
      "46464 46592\n",
      "46592 46720\n",
      "46720 46848\n",
      "46848 46976\n",
      "46976 47104\n",
      "47104 47232\n",
      "47232 47360\n",
      "47360 47488\n",
      "47488 47616\n",
      "47616 47744\n",
      "47744 47872\n",
      "47872 48000\n",
      "48000 48128\n",
      "48128 48256\n",
      "48256 48384\n",
      "48384 48512\n",
      "48512 48640\n",
      "48640 48768\n",
      "48768 48896\n",
      "48896 49024\n",
      "49024 49152\n",
      "49152 49280\n",
      "49280 49408\n",
      "49408 49536\n",
      "49536 49664\n",
      "49664 49792\n",
      "49792 49920\n"
     ]
    }
   ],
   "source": [
    "batch = 128\n",
    "for i,j in training_batch:\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}